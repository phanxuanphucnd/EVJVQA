{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from tqdm import tqdm\n",
    "from ofa.ofa_infer import OFAInference\n",
    "from evaluate_metrics import compute_f1\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06ee8a",
   "metadata": {},
   "source": [
    "## ViLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad41de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "def infer_vilt(image_path, question):\n",
    "    image = Image.open(image_path)\n",
    "    # prepare inputs\n",
    "    encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    idx = logits.argmax(-1).item()\n",
    "    \n",
    "    answer = model.config.id2label[idx]\n",
    "    \n",
    "    split_answer = answer.split()\n",
    "    answ = []\n",
    "    for ans in split_answer:\n",
    "        try:\n",
    "            answ.append(num2words(ans).replace('-', ' '))\n",
    "        except:\n",
    "            answ.append(ans)\n",
    "    \n",
    "    return ' '.join(answ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911f449",
   "metadata": {},
   "source": [
    "## OFA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520131e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phucpx/miniconda3/envs/text/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/phucpx/miniconda3/envs/text/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "ofa_base = OFAInference()\n",
    "\n",
    "def infer_ofa_base(image_path, question):\n",
    "    answer = ofa_base.ofa_inference(image_path, question)\n",
    "    split_ans = answer.split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17de581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phucpx/miniconda3/envs/text/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/phucpx/miniconda3/envs/text/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "# ofa = OFAInference()\n",
    "\n",
    "ofa = OFAInference(pretrained_path='models/vqa_large_best.pt')\n",
    "\n",
    "def infer_ofa(image_path, question):\n",
    "    answer = ofa.ofa_inference(image_path, question)\n",
    "    split_ans = answer.split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bc20f",
   "metadata": {},
   "source": [
    "## LAVIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdd785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bb21943dfb423e8fe998b78b722e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9fdc708571480486fdf903bcc6d2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3852a539d0064066a91ef784eb6bcd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3166f498d22b4b0c934148793d8430b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(\n",
    "    name=\"blip_vqa\", model_type=\"vqav2\", is_eval=True, device=device)\n",
    "\n",
    "def infer_lavis(image_path, question):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    question = txt_processors[\"eval\"](question)\n",
    "    answer = model.predict_answers(\n",
    "        samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\n",
    "    \n",
    "    split_ans = answer[0].split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test/evjvqa_public_test-lang-qtype-answer.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "annotations = test_data['annotations']\n",
    "\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_answers = []\n",
    "ofa_base_answers = []\n",
    "ofa_answers = []\n",
    "vilt_answers = []\n",
    "lavis_answers = []\n",
    "\n",
    "gold_dict = {}\n",
    "ofa_base_dict = {}\n",
    "ofa_dict = {}\n",
    "vilt_dict = {}\n",
    "lavis_dict = {}\n",
    "\n",
    "\n",
    "for anno in tqdm(annotations):\n",
    "    if anno['question_type'] in ['OTHERS'] and anno['language'] == 'en':\n",
    "#         vilt_answers.append(infer_vilt(anno['img_path'], anno['question']))\n",
    "#         ofa_base_answers.append(infer_ofa_base(anno['img_path'], anno['question']))\n",
    "#         ofa_answers.append(infer_ofa(anno['img_path'], anno['question']))\n",
    "        lavis_answers.append(infer_lavis(anno['img_path'], anno['question']))\n",
    "        gold_answers.append(anno['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for anno in tqdm(annotations):\n",
    "    if anno['question_type'] in ['OTHERS'] and anno['language'] == 'en':\n",
    "        idx = annotations[i]['id']\n",
    "        gold_dict[idx] = gold_answers[i]\n",
    "#         ofa_base_dict[idx] = ofa_base_answers[i]\n",
    "#         ofa_dict[idx] = ofa_answers[i]\n",
    "#         vilt_dict[idx] = vilt_answers[i]\n",
    "        lavis_dict[idx] = lavis_answers[i]\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1703c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofa_score: 32/216~=0.14814814814814814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ofa_base_score = 0\n",
    "ofa_score = 0\n",
    "vilt_score = 0\n",
    "lavis_score = 0\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "#     try:\n",
    "#     if ofa_answers[i] in gold_answers[i]:\n",
    "#         ofa_score += 1\n",
    "    \n",
    "#     if ofa_base_answers[i] in gold_answers[i]:\n",
    "#         ofa_base_score += 1\n",
    "    \n",
    "#     if vilt_answers[i] in gold_answers[i]:\n",
    "#         vilt_score += 1\n",
    "        \n",
    "    if lavis_answers[i] in gold_answers[i]:\n",
    "        lavis_score += 1\n",
    "        \n",
    "# (ofa_score, ofa_base_score, vilt_score), (ofa_score / i, ofa_base_score/i, vilt_score/i)\n",
    "\n",
    "print(\n",
    "#     f\"vilt_score: {vilt_score}/{len(gold_answers)}~={vilt_score/len(gold_answers)}\\n\"\n",
    "#     f\"ofa_score: {ofa_score}/{len(gold_answers)}~={ofa_score/ len(gold_answers)}\\n\"\n",
    "#     f\"ofa_base_score: {ofa_base_score}/{len(gold_answers)}~={ofa_base_score/len(gold_answers)}\\n\"\n",
    "    f\"lavis_score {lavis_score}/{len(gold_answers)}~={lavis_score/len(gold_answers)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d553d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFA: 0.15776401622527356\n"
     ]
    }
   ],
   "source": [
    "# print(f\"OFA: {compute_f1(a_gold=gold_dict, a_pred=ofa_dict)}\")\n",
    "\n",
    "# print(f\"OFA-base: {compute_f1(a_gold=gold_dict, a_pred=ofa_base_dict)}\")\n",
    "\n",
    "# print(f\"ViLT: {compute_f1(a_gold=gold_dict, a_pred=vilt_dict)}\")\n",
    "\n",
    "print(f\"BLIP: {compute_f1(a_gold=gold_dict, a_pred=lavis_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f0ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
