{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from num2words import num2words\n",
    "from ofa.ofa_infer import OFAInference\n",
    "from evaluate_metrics import compute_f1\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911f449",
   "metadata": {},
   "source": [
    "## OFA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17de581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phucpx/miniconda3/envs/phucpx/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/phucpx/miniconda3/envs/phucpx/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "# ofa = OFAInference()\n",
    "\n",
    "ofa = OFAInference(pretrained_path='models/ofa_huge.pt')\n",
    "\n",
    "def infer_ofa(image_path, question):\n",
    "    answer = ofa.ofa_inference(image_path, question)\n",
    "    split_ans = answer.split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bc20f",
   "metadata": {},
   "source": [
    "## LAVIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cdd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(\n",
    "    name=\"blip_vqa\", model_type=\"aokvqa\", is_eval=True, device=device)\n",
    "\n",
    "def infer_lavis(image_path, question):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    question = txt_processors[\"eval\"](question)\n",
    "    answer = model.predict_answers(\n",
    "        samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\n",
    "    \n",
    "    split_ans = answer[0].split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e6cea",
   "metadata": {},
   "source": [
    "## Public-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4533e0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5015"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/test/evjvqa_public_test-lang-qtype-answer.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "annotations = test_data['annotations']\n",
    "\n",
    "print(len(annotations))\n",
    "\n",
    "gold_answers = []\n",
    "ofa_answers = []\n",
    "vilt_answers = []\n",
    "lavis_answers = []\n",
    "\n",
    "\n",
    "for anno in tqdm(annotations):\n",
    "#     if anno['question_type'] in ['WHAT_COLOR'] and anno['language'] == 'en':\n",
    "    if anno['language'] == 'en':\n",
    "#         vilt_answers.append(infer_vilt(anno['img_path'], anno['question']))\n",
    "#         ofa_answers.append(infer_ofa(anno['img_path'], anno['question']))\n",
    "        lavis_answers.append(infer_lavis(anno['img_path'], anno['question']))\n",
    "        gold_answers.append(anno['answer'])\n",
    "        \n",
    "i = 0\n",
    "gold_dict = {}\n",
    "ofa_dict = {}\n",
    "vilt_dict = {}\n",
    "lavis_dict = {}\n",
    "\n",
    "for j, anno in tqdm(enumerate(annotations)):\n",
    "#     if anno['question_type'] in ['WHAT_COLOR'] and anno['language'] == 'en':\n",
    "    if anno['language'] == 'en':\n",
    "        idx = annotations[j]['id']\n",
    "        gold_dict[idx] = gold_answers[i]\n",
    "#         ofa_dict[idx] = ofa_answers[i]\n",
    "#         vilt_dict[idx] = vilt_answers[i]\n",
    "        lavis_dict[idx] = lavis_answers[i]\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37bf87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./outputs/results-lavis-aokvqa.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(lavis_dict, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfdda6",
   "metadata": {},
   "source": [
    "## Private-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d71f057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c729e66653e4e32a514088700248092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('data/private-test/evjvqa_private_test-desc-lang-qtype.json', 'r', encoding='utf-8') as f:\n",
    "    ptest_data = json.load(f)\n",
    "    \n",
    "annotations = ptest_data['annotations']\n",
    "\n",
    "print(len(annotations))\n",
    "\n",
    "gold_answers = []\n",
    "ofa_answers = []\n",
    "vilt_answers = []\n",
    "lavis_answers = []\n",
    "\n",
    "\n",
    "for anno in tqdm(annotations):\n",
    "#     if anno['question_type'] in ['WHAT_COLOR'] and anno['language'] == 'en':\n",
    "    if anno['language'] == 'en':\n",
    "#         vilt_answers.append(infer_vilt(anno['img_path'], anno['question']))\n",
    "#         ofa_answers.append(infer_ofa(anno['img_path'], anno['question']))\n",
    "        lavis_answers.append(infer_lavis(anno['img_path'], anno['question']))\n",
    "        gold_answers.append(anno['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0816ce38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2236ab883584ca28262b883c2deaa50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3343"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "gold_dict = {}\n",
    "ofa_dict = {}\n",
    "vilt_dict = {}\n",
    "lavis_dict = {}\n",
    "\n",
    "for j, anno in tqdm(enumerate(annotations)):\n",
    "#     if anno['question_type'] in ['WHAT_COLOR'] and anno['language'] == 'en':\n",
    "    if anno['language'] == 'en':\n",
    "        idx = annotations[j]['id']\n",
    "        gold_dict[idx] = gold_answers[i]\n",
    "#         ofa_dict[idx] = ofa_answers[i]\n",
    "        lavis_dict[idx] = lavis_answers[i]\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90473b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./outputs/private-test/results-lavis-aokvqa.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(lavis_dict, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512552a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phucpx",
   "language": "python",
   "name": "phucpx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
