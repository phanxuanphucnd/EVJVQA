{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from tqdm.notebook import tqdm\n",
    "from ofa.ofa_infer import OFAInference\n",
    "from evaluate_metrics import compute_f1\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06ee8a",
   "metadata": {},
   "source": [
    "## ViLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad41de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHICHprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "def infer_vilt(image_path, question):\n",
    "    image = Image.open(image_path)\n",
    "    # prepare inputs\n",
    "    encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    idx = logits.argmax(-1).item()\n",
    "    \n",
    "    answer = model.config.id2label[idx]\n",
    "    \n",
    "    split_answer = answer.split()\n",
    "    answ = []\n",
    "    for ans in split_answer:\n",
    "        try:\n",
    "            answ.append(num2words(ans).replace('-', ' '))\n",
    "        except:\n",
    "            answ.append(ans)\n",
    "    \n",
    "    return ' '.join(answ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911f449",
   "metadata": {},
   "source": [
    "## OFA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17de581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phucpx/miniconda3/envs/phucpx/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/phucpx/miniconda3/envs/phucpx/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "# ofa = OFAInference()\n",
    "\n",
    "ofa = OFAInference(pretrained_path='models/vqa_huge_best.pt')\n",
    "\n",
    "def infer_ofa(image_path, question):\n",
    "    answer = ofa.ofa_inference(image_path, question)\n",
    "    split_ans = answer.split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bc20f",
   "metadata": {},
   "source": [
    "## LAVIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8cdd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(\n",
    "    name=\"blip_vqa\", model_type=\"aokvqa\", is_eval=True, device=device)\n",
    "\n",
    "def infer_lavis(image_path, question):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    question = txt_processors[\"eval\"](question)\n",
    "    answer = model.predict_answers(\n",
    "        samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\n",
    "    \n",
    "    split_ans = answer[0].split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4533e0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5015"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/test/evjvqa_public_test-lang-qtype-answer.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "annotations = test_data['annotations']\n",
    "\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ca54ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1f25c37e3745119501ccaab1a13584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gold_answers = []\n",
    "ofa_answers = []\n",
    "vilt_answers = []\n",
    "lavis_answers = []\n",
    "\n",
    "\n",
    "for anno in tqdm(annotations):\n",
    "    if anno['question_type'] in ['WHAT_COLOR'] and anno['language'] == 'en':\n",
    "#     if anno['language'] == 'en':\n",
    "#         vilt_answers.append(infer_vilt(anno['img_path'], anno['question']))\n",
    "#         ofa_answers.append(infer_ofa(anno['img_path'], anno['question']))\n",
    "        lavis_answers.append(infer_lavis(anno['img_path'], anno['question']))\n",
    "        gold_answers.append(anno['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e4c705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f21e5ef44384d6d94338313ddb6f989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "gold_dict = {}\n",
    "ofa_dict = {}\n",
    "vilt_dict = {}\n",
    "lavis_dict = {}\n",
    "\n",
    "for j, anno in tqdm(enumerate(annotations)):\n",
    "    if anno['question_type'] in ['WHAT_COLOR'] and anno['language'] == 'en':\n",
    "#     if anno['language'] == 'en':\n",
    "        idx = annotations[j]['id']\n",
    "        gold_dict[idx] = gold_answers[i]\n",
    "#         ofa_dict[idx] = ofa_answers[i]\n",
    "#         vilt_dict[idx] = vilt_answers[i]\n",
    "        lavis_dict[idx] = lavis_answers[i]\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37bf87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./outputs/results-lavis-best-en.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(lavis_dict, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "# with open('./outputs/results-lavis-best-en.json', 'r', encoding='utf-8') as f:\n",
    "#     ofa_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b728c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_lavis_dict = {}\n",
    "for k, v in lavis_dict.items():\n",
    "    tmp_ofa_dict[int(k)] = v\n",
    "    \n",
    "lavis_dict = tmp_lavis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1703c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lavis_score 152/212~=71.69811320754717\n",
      "OFA: 0.47023037235301385\n"
     ]
    }
   ],
   "source": [
    "ofa_score = 0\n",
    "vilt_score = 0\n",
    "lavis_score = 0\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    \n",
    "#     if ofa_answers[i] in gold_answers[i]:\n",
    "#         ofa_score += 1\n",
    "    \n",
    "#     if vilt_answers[i] in gold_answers[i]:\n",
    "#         vilt_score += 1\n",
    "        \n",
    "    if lavis_answers[i] in gold_answers[i]:\n",
    "        lavis_score += 1\n",
    "        \n",
    "# (ofa_score, ofa_base_score, vilt_score), (ofa_score / i, ofa_base_score/i, vilt_score/i)\n",
    "\n",
    "print(\n",
    "#     f\"vilt_score: {vilt_score}/{len(gold_answers)}~={vilt_score/len(gold_answers) * 100}\\n\"\n",
    "#     f\"ofa_score: {ofa_score}/{len(gold_answers)}~={ofa_score/ len(gold_answers) * 100}\\n\"\n",
    "    f\"lavis_score {lavis_score}/{len(gold_answers)}~={lavis_score/len(gold_answers) * 100}\"\n",
    ")\n",
    "\n",
    "print(f\"OFA: {compute_f1(a_gold=gold_dict, a_pred=ofa_dict)}\")\n",
    "\n",
    "# print(f\"ViLT: {compute_f1(a_gold=gold_dict, a_pred=vilt_dict)}\")\n",
    "\n",
    "# print(f\"BLIP: {compute_f1(a_gold=gold_dict, a_pred=lavis_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFA: 0.26887240779732646\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phucpx",
   "language": "python",
   "name": "phucpx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
