{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from tqdm import tqdm\n",
    "from ofa.ofa_infer import OFAInference\n",
    "from evaluate_metrics import compute_acc\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06ee8a",
   "metadata": {},
   "source": [
    "## ViLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad41de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "def infer_vilt(image_path, question):\n",
    "    image = Image.open(image_path)\n",
    "    # prepare inputs\n",
    "    encoding = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    idx = logits.argmax(-1).item()\n",
    "    \n",
    "    answer = model.config.id2label[idx]\n",
    "    \n",
    "    split_answer = answer.split()\n",
    "    answ = []\n",
    "    for ans in split_answer:\n",
    "        try:\n",
    "            answ.append(num2words(ans).replace('-', ' '))\n",
    "        except:\n",
    "            answ.append(ans)\n",
    "    \n",
    "    return ' '.join(answ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911f449",
   "metadata": {},
   "source": [
    "## OFA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520131e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phucpx/miniconda3/envs/phucpx/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/phucpx/miniconda3/envs/phucpx/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "ofa_base = OFAInference()\n",
    "\n",
    "def infer_ofa_base(image_path, question):\n",
    "    answer = ofa_base.ofa_inference(image_path, question)\n",
    "    split_ans = answer.split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17de581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phucpx/miniconda3/envs/phucpx/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/phucpx/miniconda3/envs/phucpx/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "# ofa = OFAInference()\n",
    "\n",
    "ofa = OFAInference(pretrained_path='models/vqa_large_best.pt')\n",
    "\n",
    "def infer_ofa(image_path, question):\n",
    "    answer = ofa.ofa_inference(image_path, question)\n",
    "    split_ans = answer.split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bc20f",
   "metadata": {},
   "source": [
    "## LAVIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8cdd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(\n",
    "    name=\"blip_vqa\", model_type=\"vqav2\", is_eval=True, device=device)\n",
    "\n",
    "def infer_lavis(image_path, question):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    question = txt_processors[\"eval\"](question)\n",
    "    answer = model.predict_answers(\n",
    "        samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\n",
    "    \n",
    "    split_ans = answer[0].split()\n",
    "    ans = []\n",
    "    for w in split_ans:\n",
    "        try:\n",
    "            ans.append(num2words(w))\n",
    "        except:\n",
    "            ans.append(w)\n",
    "    return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4533e0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23785"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/train/evjvqa_train_lang_qtype-detailed.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "annotations = train_data['annotations']\n",
    "\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ca54ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23785/23785 [01:34<00:00, 250.94it/s]\n"
     ]
    }
   ],
   "source": [
    "gold_answers = []\n",
    "ofa_base_answers = []\n",
    "ofa_answers = []\n",
    "vilt_answers = []\n",
    "lavis_answers = []\n",
    "\n",
    "\n",
    "for anno in tqdm(annotations):\n",
    "    if anno['question_type'] in ['WHAT_DO'] and anno['language'] == 'en':\n",
    "        vilt_answers.append(infer_vilt(anno['img_path'], anno['question']))\n",
    "#         ofa_base_answers.append(infer_ofa_base(anno['img_path'], anno['question']))\n",
    "#         ofa_answers.append(infer_ofa(anno['img_path'], anno['question']))\n",
    "#         lavis_answers.append(infer_lavis(anno['img_path'], anno['question']))\n",
    "        gold_answers.append(anno['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468709ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dict = {}\n",
    "ofa_base_dict = {}\n",
    "ofa_dict = {}\n",
    "vilt_dict = {}\n",
    "lavis_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e4c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23785/23785 [00:00<00:00, 4574118.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for anno in tqdm(annotations):\n",
    "    if anno['question_type'] in ['WHAT_DO'] and anno['language'] == 'en':\n",
    "        idx = annotations[i]['id']\n",
    "        gold_dict[idx] = gold_answers[i]\n",
    "#         ofa_base_dict[idx] = ofa_base_answers[i]\n",
    "#         ofa_dict[idx] = ofa_answers[i]\n",
    "        vilt_dict[idx] = vilt_answers[i]\n",
    "#         lavis_dict[idx] = lavis_answers[i]\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf58dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vilt_score: 127/1029~=0.12342079689018465\n",
      "lavis_score 0/1029~=0.0\n"
     ]
    }
   ],
   "source": [
    "ofa_base_score = 0\n",
    "ofa_score = 0\n",
    "vilt_score = 0\n",
    "lavis_score = 0\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "#     try:\n",
    "#     if ofa_answers[i] in gold_answers[i]:\n",
    "#         ofa_score += 1\n",
    "    \n",
    "#     if ofa_base_answers[i] in gold_answers[i]:\n",
    "#         ofa_base_score += 1\n",
    "    \n",
    "    if vilt_answers[i] in gold_answers[i]:\n",
    "        vilt_score += 1\n",
    "        \n",
    "#     if lavis_answers[i] in gold_answers[i]:\n",
    "#         lavis_score += 1\n",
    "        \n",
    "# (ofa_score, ofa_base_score, vilt_score), (ofa_score / i, ofa_base_score/i, vilt_score/i)\n",
    "\n",
    "print(\n",
    "    f\"vilt_score: {vilt_score}/{i}~={vilt_score/i}\\n\"\n",
    "#     f\"ofa_score: {ofa_score}/{i}~={ofa_score/i}\\n\"\n",
    "#     f\"ofa_base_score: {ofa_base_score}/{i}~={ofa_base_score/i}\\n\"\n",
    "    f\"lavis_score {lavis_score}/{i}~={lavis_score/i}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86d553d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_acc(a_gold=gold_dict, a_pred=ofa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8165cd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19624572550596328"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_acc(a_gold=gold_dict, a_pred=ofa_base_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62fa1cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04639319261219669"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_acc(a_gold=gold_dict, a_pred=vilt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5eca868c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1791443713339354"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_acc(a_gold=gold_dict, a_pred=lavis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f0ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phucpx",
   "language": "python",
   "name": "phucpx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
